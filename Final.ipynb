{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d08f2d6-cbe9-4802-9ac6-eba873e11517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Step 0, Loss: 2.5957894325256348\n",
      "Step 10, Loss: 1.980647087097168\n",
      "Step 20, Loss: 1.6816656589508057\n",
      "Step 30, Loss: 2.129777431488037\n",
      "Step 40, Loss: 1.3703655004501343\n",
      "Epoch 1 completed. Total Loss: 144.40478575229645\n",
      "Starting Epoch 1/10\n",
      "Step 0, Loss: 7.667163848876953, Reward: 0.4864864864864865, Smoothed Reward: 0.38648648648648654\n",
      "Step 10, Loss: 0.011640017852187157, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 0.011866576038300991, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 30, Loss: 0.011917616240680218, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 40, Loss: 0.011769535019993782, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Epoch 1 completed. Total Loss: 21.425146535038948\n",
      "Starting Epoch 2/10\n",
      "Step 0, Loss: 0.011574244126677513, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 10, Loss: 0.011576224118471146, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 0.011369286105036736, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 30, Loss: 0.011839451268315315, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 40, Loss: 0.011370339430868626, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Epoch 2 completed. Total Loss: 0.6840974260121584\n",
      "Starting Epoch 3/10\n",
      "Step 0, Loss: 0.011227104812860489, Reward: 0.06060606060606061, Smoothed Reward: 0.001\n",
      "Step 10, Loss: 0.01128450222313404, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 0.011143107898533344, Reward: 0.05555555555555555, Smoothed Reward: 0.001\n",
      "Step 30, Loss: 0.5973799228668213, Reward: 0.15384615384615383, Smoothed Reward: 0.05384615384615382\n",
      "Step 40, Loss: 0.011019019410014153, Reward: 0.08888888888888889, Smoothed Reward: 0.001\n",
      "Epoch 3 completed. Total Loss: 7.505556605756283\n",
      "Starting Epoch 4/10\n",
      "Step 0, Loss: 0.12454351782798767, Reward: 0.1111111111111111, Smoothed Reward: 0.0111111111111111\n",
      "Step 10, Loss: 0.010993004776537418, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 1.2117457389831543, Reward: 0.21052631578947367, Smoothed Reward: 0.11052631578947367\n",
      "Step 30, Loss: 0.010937445797026157, Reward: 0.05263157894736842, Smoothed Reward: 0.001\n",
      "Step 40, Loss: 0.010946499183773994, Reward: 0.09523809523809525, Smoothed Reward: 0.001\n",
      "Epoch 4 completed. Total Loss: 17.352061460725963\n",
      "Starting Epoch 5/10\n",
      "Step 0, Loss: 0.010939700528979301, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 10, Loss: 0.010965232737362385, Reward: 0.0, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 0.01094126608222723, Reward: 0.04651162790697675, Smoothed Reward: 0.001\n",
      "Step 30, Loss: 0.6330456733703613, Reward: 0.15789473684210528, Smoothed Reward: 0.05789473684210528\n",
      "Step 40, Loss: 0.23171396553516388, Reward: 0.12121212121212122, Smoothed Reward: 0.02121212121212121\n",
      "Epoch 5 completed. Total Loss: 4.985872220247984\n",
      "Starting Epoch 6/10\n",
      "Step 0, Loss: 0.8348016142845154, Reward: 0.17647058823529413, Smoothed Reward: 0.07647058823529412\n",
      "Step 10, Loss: 0.05751829966902733, Reward: 0.10526315789473685, Smoothed Reward: 0.005263157894736845\n",
      "Step 20, Loss: 0.057413019239902496, Reward: 0.10526315789473684, Smoothed Reward: 0.005263157894736831\n",
      "Step 30, Loss: 1.1467052698135376, Reward: 0.20512820512820512, Smoothed Reward: 0.10512820512820512\n",
      "Step 40, Loss: 0.057353675365448, Reward: 0.10526315789473684, Smoothed Reward: 0.005263157894736831\n",
      "Epoch 6 completed. Total Loss: 21.133525393903255\n",
      "Starting Epoch 7/10\n",
      "Step 0, Loss: 0.7274896502494812, Reward: 0.16666666666666669, Smoothed Reward: 0.06666666666666668\n",
      "Step 10, Loss: 0.0109002236276865, Reward: 0.08695652173913043, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 0.010885016061365604, Reward: 0.1, Smoothed Reward: 0.001\n",
      "Step 30, Loss: 1.776024580001831, Reward: 0.2631578947368421, Smoothed Reward: 0.16315789473684209\n",
      "Step 40, Loss: 0.057278476655483246, Reward: 0.10526315789473684, Smoothed Reward: 0.005263157894736831\n",
      "Epoch 7 completed. Total Loss: 24.66792325116694\n",
      "Starting Epoch 8/10\n",
      "Step 0, Loss: 0.8324022889137268, Reward: 0.17647058823529413, Smoothed Reward: 0.07647058823529412\n",
      "Step 10, Loss: 0.010887295007705688, Reward: 0.06896551724137931, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 1.2644456624984741, Reward: 0.21621621621621623, Smoothed Reward: 0.11621621621621622\n",
      "Step 30, Loss: 1.9339008331298828, Reward: 0.2777777777777778, Smoothed Reward: 0.17777777777777778\n",
      "Step 40, Loss: 0.1553524285554886, Reward: 0.11428571428571428, Smoothed Reward: 0.014285714285714277\n",
      "Epoch 8 completed. Total Loss: 25.54281148687005\n",
      "Starting Epoch 9/10\n",
      "Step 0, Loss: 0.629631519317627, Reward: 0.15789473684210525, Smoothed Reward: 0.05789473684210525\n",
      "Step 10, Loss: 0.010891719721257687, Reward: 0.0909090909090909, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 1.201444387435913, Reward: 0.21052631578947367, Smoothed Reward: 0.11052631578947367\n",
      "Step 30, Loss: 1.9320255517959595, Reward: 0.2777777777777778, Smoothed Reward: 0.17777777777777778\n",
      "Step 40, Loss: 0.5852757692337036, Reward: 0.15384615384615385, Smoothed Reward: 0.05384615384615385\n",
      "Epoch 9 completed. Total Loss: 28.444584297016263\n",
      "Starting Epoch 10/10\n",
      "Step 0, Loss: 0.6759672164916992, Reward: 0.16216216216216214, Smoothed Reward: 0.06216216216216214\n",
      "Step 10, Loss: 0.010877949185669422, Reward: 0.08333333333333333, Smoothed Reward: 0.001\n",
      "Step 20, Loss: 1.2010442018508911, Reward: 0.21052631578947367, Smoothed Reward: 0.11052631578947367\n",
      "Step 30, Loss: 1.6303361654281616, Reward: 0.25, Smoothed Reward: 0.15\n",
      "Step 40, Loss: 0.9506403207778931, Reward: 0.18749999999999997, Smoothed Reward: 0.08749999999999997\n",
      "Epoch 10 completed. Total Loss: 25.32515128608793\n",
      "Evaluation ROUGE Scores:\n",
      "rouge1: 0.19473045935780958\n",
      "rouge2: 0.003277591973244147\n",
      "rougeL: 0.13194897728520433\n",
      "rougeLsum: 0.13143147017285256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Load the XSum dataset (subset for demonstration)\n",
    "dataset = load_dataset(\"xsum\", split='train[:100]')\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Compute reward using ROUGE scores\n",
    "def compute_reward(preds, references):\n",
    "    \"\"\"\n",
    "    Compute the reward using ROUGE scores.\n",
    "    \"\"\"\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=references)\n",
    "    reward = rouge_scores[\"rouge1\"]  # Use ROUGE-1 F1 score as reward\n",
    "    return reward\n",
    "\n",
    "# Supervised fine-tuning function (optional warm-up)\n",
    "def supervised_fine_tune(model, tokenizer, dataset, epochs=10, lr=5e-5):\n",
    "    \"\"\"\n",
    "    Supervised fine-tuning of the model using labeled data.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "            input_text = data['document']\n",
    "            reference_summary = data['summary']\n",
    "\n",
    "            # Prepare inputs and labels\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(device)\n",
    "            labels = tokenizer(reference_summary, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).input_ids.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=inputs['input_ids'], labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}, Loss: {loss.item()}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed. Total Loss: {total_loss}\")\n",
    "\n",
    "# Fine-tuning with REINFORCE and Baseline Reward Smoothing\n",
    "def fine_tune_with_reinforce(model, tokenizer, dataset, epochs=3, lr=5e-5, baseline_reward=0.1):\n",
    "    \"\"\"\n",
    "    Fine-tune the model using REINFORCE with smoothed rewards and baseline subtraction.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            input_text = data['document']\n",
    "            reference_summary = data['summary']\n",
    "\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(device)\n",
    "\n",
    "            # Generate summary\n",
    "            generated_ids = model.generate(\n",
    "                inputs['input_ids'], max_length=60, min_length=10, length_penalty=2.0, num_beams=4\n",
    "            )\n",
    "            generated_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Compute reward\n",
    "            reward = compute_reward([generated_summary], [reference_summary])\n",
    "            smoothed_reward = max(reward - baseline_reward, 1e-3)  # Reward smoothing and baseline subtraction\n",
    "\n",
    "            # Compute log probabilities of the generated summary\n",
    "            outputs = model(input_ids=inputs['input_ids'], labels=generated_ids)\n",
    "            logits = outputs.logits\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # Policy gradient loss\n",
    "            loss = -smoothed_reward * log_probs.mean()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}, Loss: {loss.item()}, Reward: {reward}, Smoothed Reward: {smoothed_reward}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed. Total Loss: {total_loss}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the fine-tuned model using ROUGE scores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for data in dataset:\n",
    "        input_text = data['document']\n",
    "        reference_summary = data['summary']\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(device)\n",
    "\n",
    "        # Generate summary\n",
    "        generated_ids = model.generate(\n",
    "            inputs['input_ids'], max_length=60, min_length=10, length_penalty=2.0, num_beams=4\n",
    "        )\n",
    "        generated_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(generated_summary)\n",
    "        references.append(reference_summary)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    print(\"Evaluation ROUGE Scores:\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the training process\n",
    "train_data = dataset.select(range(50))  # Subset for fine-tuning\n",
    "val_data = dataset.select(range(50, 100))  # Subset for evaluation\n",
    "\n",
    "# Optional supervised fine-tuning\n",
    "supervised_fine_tune(model, tokenizer, train_data, epochs=1, lr=5e-5)\n",
    "\n",
    "# Reinforce fine-tuning with baseline and smoothing\n",
    "fine_tune_with_reinforce(model, tokenizer, train_data, epochs=10, lr=5e-5, baseline_reward=0.1)\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "evaluate_model(model, tokenizer, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf9b88-ef56-419d-ac11-538595dc5f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
